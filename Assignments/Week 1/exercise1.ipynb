{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodal Deep Learning Exercise 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning models learn hierarchical features. Each layer can be seen as performing a coordinate transformation. Non-linear activations allow these transformations to \"unfold\" the data manifold. In this exercise, you will implement and visualize this\n",
    "process using a simple dataset and a Multi-Layer Perceptron (MLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert your code wherever you see ``` # TODO:``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "##### Do not modify this code #####\n",
    "###################################\n",
    "\n",
    "def generate_3d_spirals(n_points_per_class=500, noise=0.2):\n",
    "    \"\"\"Generates two intertwined 3D spirals.\"\"\"\n",
    "    # Spiral 1\n",
    "    theta1 = np.sqrt(np.random.rand(n_points_per_class)) * 3 * np.pi # angle\n",
    "    r1 = theta1 + noise * np.random.randn(n_points_per_class) # radius variation\n",
    "    x1 = r1 * np.cos(theta1)\n",
    "    y1 = r1 * np.sin(theta1)\n",
    "    z1 = theta1 * 0.5 + noise * np.random.randn(n_points_per_class) # z increases with angle\n",
    "    class1 = np.zeros(n_points_per_class)\n",
    "\n",
    "    # Spiral 2 (offset phase and direction)\n",
    "    theta2 = np.sqrt(np.random.rand(n_points_per_class)) * 3 * np.pi # angle\n",
    "    r2 = theta2 + noise * np.random.randn(n_points_per_class) # radius variation\n",
    "    x2 = -r2 * np.cos(theta2) # Negate to interwine\n",
    "    y2 = -r2 * np.sin(theta2) # Negate to interwine\n",
    "    z2 = theta2 * 0.5 + noise * np.random.randn(n_points_per_class) # z increases with angle\n",
    "    class2 = np.ones(n_points_per_class)\n",
    "\n",
    "    # Combine\n",
    "    X = np.vstack((np.column_stack((x1, y1, z1)), np.column_stack((x2, y2, z2))))\n",
    "    y = np.concatenate((class1, class2))\n",
    "\n",
    "    # Shuffle\n",
    "    idx = np.random.permutation(len(y))\n",
    "    X, y = X[idx], y[idx]\n",
    "\n",
    "    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "###################################\n",
    "###################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "X, y = generate_3d_spirals(n_points_per_class=500, noise=0.05)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Data shape:\", X_train.shape)\n",
    "print(\"Labels shape:\", y_train.shape)\n",
    "\n",
    "# Move data to device\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize in a 3D scatter plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the simplest possible neural network: a single linear layer mapping the 3D input directly to a 1D output (logit for binary classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the Model\n",
    "\n",
    "# Should have one Linear layer mapping 3 input features to 1 output feature.\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # TODO: Define the linear layer (input_dim=3, output_dim=1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Pass input through the linear layer\n",
    "\n",
    "\n",
    "# --- Training Setup ---\n",
    "model1 = LinearModel().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model1.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "epochs = 2000\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model1.train()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model1(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot original data points colored by the class labels predicted by model 1\n",
    "# You can put the plot of true class labels side-by-side for comparisson.\n",
    "# Optionally calculate the classification accuracy on the test set.\n",
    "# Is the linear model able to effectively separate the spiral data? \n",
    "# Remember, a linear model creates a flat separating plane in this 3D space. Can you imagine where that plane might be, based on the predicted colors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's add a hidden layer. We'll use a linear hidden layer to project the 3D data down to a 2D space *before* the final classification layer. Importantly, we will *not* use a non-linear activation function yet. The goal is to see if linear dimensionality reduction alone helps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the Model\n",
    "\n",
    "# LinearHiddenModel (3 -> 3 -> 2 -> 1)\n",
    "class LinearHiddenModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Modify this class to implement the 3 -> 3 -> 2 -> 1 linear architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim=2): # hidden_dim might be ignored, focus on 3->3->2->1\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim # Store if needed\n",
    "\n",
    "        # TODO: Define layer 1 (input=3, output=3)\n",
    "        # TODO: Define layer 2 (input=3, output=2)\n",
    "        # TODO: Define layer 3 (input=2, output=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # TODO: Pass x through layer 1\n",
    "        # TODO: Pass result through layer 2\n",
    "        # TODO: Pass result through layer 3\n",
    "        # TODO: Return the final output\n",
    "        output = x # Placeholder, replace with correct calculation\n",
    "        return output\n",
    "\n",
    "    def get_hidden_representation(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Helper function to get the output AFTER the first hidden layer (the 3 -> 3 layer).\n",
    "        \"\"\"\n",
    "        self.eval() # Ensure model is in eval mode\n",
    "        with torch.no_grad(): # Disable gradient calculations\n",
    "            # TODO: Calculate and return the output after layer 1\n",
    "            hidden_rep = x # Placeholder, replace with correct calculation\n",
    "        return hidden_rep\n",
    "\n",
    "# --- Training Setup ---\n",
    "model2 = LinearHiddenModel(hidden_dim=2).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model2.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "epochs = 2000\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model2.train()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model2(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot original data points colored by the class labels predicted by model 2\n",
    "# Optionally calculate the classification accuracy on the test set.\n",
    "# TODO: Plot the hidden representation in 2D hidden space using the helper function colored by the original and predicted class labels side-by-side.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a ReLU activation function after our hidden layer. You can implement this inheriting or modifying the previous model or defining a new model for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLUHiddenModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Modify this class to implement the 3 -> 3(ReLU) -> 2(ReLU) -> 1 architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim=2): # hidden_dim might be ignored, focus on 3->3->2->1\n",
    "        super().__init__()\n",
    "        # self.hidden_dim = hidden_dim # Store if needed\n",
    "        # print(f\"Warning: Initializing ReLUHiddenModel for 3->3->2->1 architecture. \"\n",
    "        #       f\"The hidden_dim={hidden_dim} argument is ignored for layer sizing.\") # Optional warning\n",
    "\n",
    "        # TODO: Define layer 1 (input=3, output=3)\n",
    "        # TODO: Define the first ReLU activation function instance\n",
    "        # TODO: Define layer 2 (input=3, output=2)\n",
    "        # TODO: Define the second ReLU activation function instance (can be the same instance)\n",
    "        # TODO: Define layer 3 (input=2, output=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # TODO: Pass x through layer 1\n",
    "        # TODO: Apply the first ReLU activation\n",
    "        # TODO: Pass the result through layer 2\n",
    "        # TODO: Apply the second ReLU activation\n",
    "        # TODO: Pass the result through layer 3 (output layer)\n",
    "        # TODO: Return the final output logits\n",
    "        output = x # Placeholder, replace with correct calculation\n",
    "        return output\n",
    "\n",
    "    def get_hidden_representation(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Helper function to get the output *after* the first layer (3 -> 3)\n",
    "        AND its ReLU activation function.\n",
    "        \"\"\"\n",
    "        self.eval() # Ensure model is in eval mode\n",
    "        with torch.no_grad(): # Disable gradient calculations\n",
    "            # TODO: Calculate the output after layer 1\n",
    "            # TODO: Apply the first ReLU activation to the layer 1 output\n",
    "            # TODO: Return the activated hidden representation\n",
    "            hidden_activated = x # Placeholder, replace with correct calculation\n",
    "        return hidden_activated\n",
    "\n",
    "# --- Training Setup ---\n",
    "model3 = ReLUHiddenModel().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model3.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "epochs = 2000\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model3.train()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model3(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot original data points colored by the class labels predicted by model 3\n",
    "# Optionally calculate the classification accuracy on the test set.\n",
    "# TODO: Plot the hidden representation in 2D hidden space using the helper function colored by the original and predicted class labels side-by-side.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this 2D plot (after ReLU) to the previous one (linear only). How has the representation changed?\n",
    "\n",
    "*Optional:*\n",
    "Deep learning often involves stacking multiple layers. Let's briefly see how adding another hidden layer might further refine the representation.\n",
    "1. Define a model with more or larger layers.\n",
    "2. Train it.\n",
    "3. Visualize the output of the last hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clean_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
